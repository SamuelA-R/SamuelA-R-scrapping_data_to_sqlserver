# Web Scraping de Dados Financeiros Fundamentalista
### Objetivo: Extrair e tratar os dados, Armazen√°-los no SQL Server, analisar os dados e disponibiliz√°-los de forma clara para acesso e visualiza√ß√£o de insights utilizando Streamlit

***Tecnologias: Python (Requests, BeautifulSoup, Pandas, JSON), SQL SERVER para armazenar os dados e Streamlit (relat√≥rio para visualiza√ß√£o dos dados).***

***Tarefas:**
- Criar um scraper para coletar dados.  
- Armazenar os dados extra√≠dos em um banco do SQL Server.  
- Criar uma API ou script que consulte o banco.  
-	Analisar os dados utilizando Python e criar um relat√≥rio com Streamlit.

***Estrutura da Integra√ß√£o Python + SQL + Streamlit***
- ‚úÖ Python ‚Üí Extra√ß√£o, tratamento avan√ßado e automa√ß√£o
- ‚úÖ SQL ‚Üí Armazenamento, integra√ß√£o e otimiza√ß√£o de consultas
- ‚úÖ Streamlit ‚Üí Visualiza√ß√£o e an√°lise interativa

Fluxo do Processo:
1.	Python: Extrai, trata e carrega os dados no banco SQL.
2.	SQL: Cria, processa e otimiza consultas.
3.	Streamlit: Conecta-se ao banco e visualiza os dados.

## 1Ô∏è‚É£ Extraction and transform: Scraping com Python
### Scraping das a√ß√µes listadas na bolsa de valor
- Iniciaremos extraindo os dados de todas as a√ß√µes listadas na bolsa de valores. Para isso ser√° necess√°ria as seguintes bibliotecas:


```python
import requests
from bs4 import BeautifulSoup
from time import sleep
from datetime import datetime
import pandas as pd
import time
import json
```
    - requests: para fazer requisi√ß√µes `HTTP` ao site.

    - BeautifulSoup: para fazer o parsing do `HTML` e extrair os dados.

    - sleep (do time): para pausar o c√≥digo por um tempo definido (ex: sleep(1) pausa por 1 segundo).

    - datetime: para trabalhar com datas e horas (ex: pegar a data atual com datetime.now()).

    - pandas: para transformar os dados em um DataFrame e facilitar a an√°lise.

    - time: para pausar entre requisi√ß√µes (evitando bloqueios) e medir tempo de execu√ß√£o.

    - json: para trabalhar com dados no formato `JSON` (como salvar e carregar dicion√°rios).
  
- Precisamos definir a `URL` da p√°gina que ser√° acessada e os cabe√ßalhos para simular um navegador s√£o definidos, no meu caso:
```python
url = "https://www.dadosdemercado.com.br/acoes"
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}
```
- O c√≥digo abaixo vai fazer a requisi√ß√£o do `HTML` para obter as informa√ß√µes da p√°gina e verificar se a requisi√ß√£o foi bem-sucedida (`status 200`) antes de prosseguir. Para isso, o conte√∫do HTML da p√°gina √© processado com o BeautifulSoup para facilitar a extra√ß√£o dos dados:
 ```python
page = requests.get(url, headers=headers)
if page.status_code == 200:
    soup = BeautifulSoup(page.text, 'html.parser')
else:
    print(f"Erro ao acessar a p√°gina: {page.status_code}")
soup = BeautifulSoup(page.text, 'html.parser')
```
- Agora, criamos uma lista para armazenar o c√≥digo de todas as a√ß√µes listadas na bolsa, que ser√° utilizada na extra√ß√£o no site Fundamentos. A fun√ß√£o for vai percorrer toda a lista, utilizando o m√©todo `find_all`, que vai encontrar todas as tags `<strong>`


```python
dados = soup.find_all('strong')
lista = []
for data in dados:
    lista.append(data.text)
```

- Salvamos os dados extraidos em um arquivo de texto `acoes.txt`, onde cada a√ß√£o √© escrita em uma nova linha:
  
```python
with open("acoes.txt", "w") as f:
    for item in lista:
        f.write(item + '\n')  # Atualiza a lista de a√ß√µes
```

### Scraping dos dados Fundamentus
- Extra√ß√£o dos dados fundamentalistas que vamos analisar
- Utilizaremos o metodo `with` para ler os tickers extraidos

```python
with open("acoes.txt", "r") as fundamentus_file:
    lista_acoes = fundamentus_file.read().split()
```
- Define o cabe√ßalho da requisi√ß√£o simulando um navegador real para evitar que o site bloqueie a requisi√ß√£o e criamos uma lista para armazenar os dados de cada a√ß√£o
```python
headers = {
    'User-Agent': 'Mozilla/5.0 (...) Safari/537.36'
}

dados_json = []
```
- O bloco de c√≥digo abaixo percorre cada ticker presente na `lista lista_acoes`, que foi lida previamente de um arquivo de texto. Para cada ticker, ele constr√≥i a `URL` correspondente √† p√°gina de detalhes dessa a√ß√£o no site Fundamentus. Em seguida, faz uma requisi√ß√£o `HTTP` usando a biblioteca `requests`, passando um cabe√ßalho customizado para simular um navegador e evitar bloqueios do site. Se a requisi√ß√£o for bem-sucedida (`c√≥digo de status 200`), o conte√∫do `HTML` da p√°gina √© processado com `BeautifulSoup`, que permite navegar e extrair as informa√ß√µes da estrutura da p√°gina.
O c√≥digo ent√£o localiza todas as tabelas com a classe `w728`, que cont√™m os dados financeiros da a√ß√£o. Ele percorre essas tabelas linha por linha e, dentro de cada linha, acessa as colunas com os nomes dos indicadores e seus respectivos valores. Os dados s√£o extra√≠dos 
em pares: o nome do indicador (`chave`) e o valor correpondente, j√° realizando um tratamento b√°sico nos textos (como remo√ß√£o de s√≠mbolos e padroniza√ß√£o num√©rica). Antes de adicionar ao dicion√°rio, o c√≥digo verifica se a chave j√° existe ‚Äî e, caso exista, renomeia a 
chave com um sufixo num√©rico para evitar sobrescrita. Ao final do processo para uma a√ß√£o, todas as informa√ß√µes coletadas s√£o armazenadas em um dicion√°rio e adicionadas √† lista `dados_json`, que armazenar√° os dados de todas as a√ß√µes consultadas.
Esse processo se repete para cada ticker da lista, com uma pequena pausa (`sleep`) ao final de cada itera√ß√£o para evitar que o site bloqueie as requisi√ß√µes por excesso de acessos em pouco tempo.
```python
# Loop sobre cada a√ß√£o para extrair os dados
for acao in lista_acoes:
    url = f"https://www.fundamentus.com.br/detalhes.php?papel={acao}"
    
    # requisi√ß√£o
    page = requests.get(url, headers=headers)
    
    if page.status_code == 200:
        soup = BeautifulSoup(page.text, 'html.parser')
        
        tabelas = soup.select("table.w728")
        
        dados_dict = {'Ticker': acao}  # Inclui o ticker como chave

        for tabela in tabelas:  
            linhas = tabela.find_all('tr')  
            for linha in linhas:  
                colunas = linha.find_all('td')  
                if colunas:
                    for i in range(0, len(colunas), 2):
                        if i + 1 < len(colunas):
                            chave = colunas[i].get_text(strip=True).replace('?', '')
                            valor = colunas[i + 1].get_text(strip=True).replace('.', '').replace(',','.').replace('%', '')
                            
                            # Verificando se a chave j√° existe no dicion√°rio e modificando o nome da chave
                            if chave in dados_dict:
                                contador = 1
                                while f"{chave}_{contador}" in dados_dict:
                                    contador += 1
                                chave = f"{chave}_{contador}"

                            dados_dict[chave] = valor  

        dados_json.append(dados_dict)
    
    else:
        print(f"Erro ao acessar {acao}: {page.status_code}")
```
- Agora, convetemos os dados para dataframe para que possamos tratalos e carregamos com pandas.
```python
json_data = json.dumps(dados_json, ensure_ascii=False)
df = pd.read_json(json_data, orient="records")
```
- Agora excluiremos as quatro primeiras linhas do dataframe que possui dados inuteis adicionadas na extra√ß√£o e excluiremos as colunas `Oscia√ß√µes` e `√∫ltimos 12 meses` e, por fim, vamos retirar os dados de a√ß√µes n√£o encontradas de nosso dataframe e salvar novamente no formato `JSON` para inserir os dados em nosso BD.
```python
# Remover as primeiras 4 linhas e resetar o √≠ndice
df = df.drop(index=df.index[0:4]).reset_index(drop=True)
df = df.drop(columns=['Oscila√ß√µes', '', '√öltimos 12 meses', ]) # Remove as colunas inuteis criadas na hora do scraping
df = df[df['Papel'].notna() & (df['Papel'] != "")] # Retira as colunas de papeis n√£o encontrados


df.to_json('dados_json2.json')
```
## 2Ô∏è‚É£ Load: Integra√ß√£o dos dados no SQL Server
- Nest√° etapa, primeiro precisamos criar o nosso banco de dados no SQL Server. Para isso deixei o c√≥digo SQL para cria√ß√£o do database e da tabela na pasta `SQL Server`.
- J√° criado o banco (voc√™ pode encontrar o comando no arquivo `CreateDatabase.sql`), iremos inserir os dados utilizando as seguintes bibliotecas:
```python
import pyodbc
import pandas as pd
```
    - pyodbc: para conectar-se ao banco de dados e inserir os dados na tabela:
    - pandas: para converter o arquivo salvo `JSON` em dataframe para que possamos interar sobre os dados e inserir em nosso `SGDB`.

- Primeiro criaremos a nossa fun√ß√£o `conecta_ao_banco` de dados
```python
def conecta_ao_banco(driver='SQL Server', server='SAMUEL\\MSSQLSERVER01', database='Dados_scraping', trusted_connection="yes"):
    string_conexao = f"DRIVER={{{driver}}};SERVER={server};DATABASE={database};Trusted_Connection={trusted_connection};"
    conexao = pyodbc.connect(string_conexao)
    cursor = conexao.cursor()
    return conexao, cursor
```
- Ela recebe como par√¢metros o `driver`, o `servidor`, o `nome do banco de dados` e se a conex√£o √© autenticada pelo Windows (`via Trusted_Connection`).
- A string de conex√£o (`string_conexao`) √© montada com esses par√¢metros e, em seguida, o pyodbc.connect √© usado para abrir a conex√£o.

- Por fim, a fun√ß√£o retorna dois objetos:
    - conexao: o objeto de conex√£o com o banco;
    - cursor: o objeto que permite executar comandos `SQL` dentro dessa conex√£o.

- Agora, vamos inserior os dados em nosso banco.
- Primeiro criaremos nossa fun√ß√£o `try` que vai "tentar" realizar a conex√£o com nosso banco.
```python
try:
    conexao, cursor = conecta_ao_banco()
    print("Conex√£o estabelecida!")
```
- Agora vamos armazenar o nome das colunas do nosso dataframe (que s√£o as mesmas que criamos no SQL Server em SQL).
``` python
nomes_colunas = ", ".join(f"[{col}]" for col in dados.columns)
```
- Agora geramos os placeholders para cada valor de coluna (placeholders s√£o espa√ßos reservados (?) usados na query SQL para inserir os valores de forma segura e autom√°tica).
```python
placeholders = ", ".join("?" for _ in dados.columns)
```
- Agora inserimos os dados do Dataframe nas colunas existentes da tabela utilizando a fun√ß√£o `FOR`:
```python
for _, row in dados.iterrows():
  # aqui estamos pegando os valores da linha e substituindo qualquer NaN por None
  valores = tuple(row[col] if pd.notna(row[col]) else None for col in dados.columns)
        
  # monta a query de inser√ß√£o
  query = f"INSERT INTO ACOES ({nomes_colunas}) VALUES ({placeholders})"
        
  # executa a query passando os valores da linha
  cursor.execute(query, valores)
  # confirma as altera√ß√µes no banco
  conexao.commit()
  print("Dados inseridos com sucesso!")
```
- Por fim, caso de erro, utilizamos o except, printando o erro para que possamos corrigir caso ocorra:
```python
except Exception as e:
    print("Erro:", e)
```

## 3Ô∏è‚É£ Dashboard: Relat√≥rio no Streamlit
- Ser√° necess√°ria as seguintes bibliotecas:

```python
import streamlit as st
import pandas as pd
import plotly.graph_objects as go
from streamlit_option_menu import option_menu
import plotly.express as px
import pyodbc
```
    Utilidades de cada biblioteca:
    - Streamlit: construir a interface web
    - Pandas: manipular os dados para an√°lise
    - Plotly (go e express): criar gr√°ficos para a an√°lise
    - Streamlit_option_menu: criar menu de bot√µes com multiplas p√°ginas
    - pyodbc: conectar-se ao banco de dados SQL Server e carregar os dados

- Iniciamos conctando ao banco de dados e realizando a query de todos os dados de nossa tabela ACOES criada. Ap√≥s isso armazenamos os dados como dataframe na vari√°vel df

```python
def conecta_ao_banco(driver='SQL Server', server='SAMUEL\\MSSQLSERVER01', database='Dados_scraping', trusted_connection="yes"):
    string_conexao = f"DRIVER={{{driver}}};SERVER={server};DATABASE={database};Trusted_Connection={trusted_connection};"
    conexao = pyodbc.connect(string_conexao)
    cursor = conexao.cursor()
    return conexao, cursor

conexao, cursor = conecta_ao_banco()
print("Conex√£o estabelecida!")
# Consultar a tabela 'ACOES' diretamente e criar o DataFrame
query = "SELECT * FROM ACOES"
df = pd.read_sql(query, conexao)
```

Ao fazermos o carregamento dos dados, primeiro convertemos as colunas n√∫mericas para o tipo float
```python
def converter_tipo(table):
    for i in df.columns:
        try:
            table[i] = table[i].astype(float)
        except ValueError:
            pass
    return table

dados = converter_tipo(df)
```
Ap√≥s isso vamos criar algumas fun√ß√µes para construir nosso dashboard:
- Fun√ß√£o para os filtros multiselect laterais:
```python
def slidebar_filtro(dados_coluna, nome_slider):
    opcoes = list(dados_coluna.unique())
    selecao = st.sidebar.multiselect(label=f"{nome_slider}", options=opcoes)
    return selecao if selecao else opcoes
```

- Fun√ß√£o que vai exibir a tabela em nosso dashboard

```python
def Home(data_selection):
    with st.expander('Tabular'):
        all_columns = data_selection.columns.tolist()
        selected_columns = st.multiselect(
            'Filter:',
            options=['Selecionar Tudo'] + all_columns,
            default=[]
        )
        showData = all_columns if 'Selecionar Tudo' in selected_columns else selected_columns
        st.write(data_selection[showData])
```
- Criaremos agora a fun√ß√£o que vai gerar nosso gr√°fico de barras, nosso gr√°fico ser√° na horizontal e hovertemplate ser√° o nosso tooltip
```python
def gerar_grafico(data, valor):
    data_filtrada = data[['Papel', valor, 'Cota√ß√£o']].dropna().sort_values(valor, ascending=False).head(20)
    fig = go.Figure(go.Bar(
        x=data_filtrada[valor],
        y=data_filtrada["Papel"],
        orientation='h',
        marker=dict(color='teal'),
        customdata=data_filtrada[['Cota√ß√£o']],
        hovertemplate='<b>Papel:</b> %{y}<br><b>' + valor + ':</b> üìà %{x}<br><b>Cota√ß√£o:</b> üí∞ %{customdata[0]}'
    ))
    fig.update_layout(
        hoverlabel=dict(bgcolor="yellow", font_color="black"),
        yaxis=dict(categoryorder="total ascending"),
        title=f"Gr√°fico de {valor}",
        height=600
    )
    return fig
```


- Agora criaremos a fun√ß√£o que vai exibir os 3 gr√°ficos lado a lado em nosso dashboard
```
def graph(df_filtrado, col1, col2=None, col3=None):
    colunas = [col1]
    if col2: colunas.append(col2)
    if col3: colunas.append(col3)

    cols = st.columns(len(colunas))
    for i, col in enumerate(colunas):
        with cols[i]:
            fig = gerar_grafico(df_filtrado, col)
            st.plotly_chart(fig, use_container_width=True)
```

- Gr√°fico de linhas da p√°gina 2
  
```python
#grafico de linhas
def line_graph(data_selection, valor):
    data = data_selection[['Papel', 'Ano', valor]].dropna()

    fig = px.line(
        data,
        x='Ano',
        y=valor,
        color='Papel',
        markers=True,
        title=f"Evolu√ß√£o de {valor} por Papel",
    )

    fig.update_traces(
        hovertemplate='<b>Papel:</b> %{fullData.name}<br><b>' + valor + ':</b> %{y}<br><b>Ano:</b> %{x}'
    )

    fig.update_layout(
        hoverlabel=dict(bgcolor="yellow"),
        height=600
    )

    st.plotly_chart(fig, use_container_width=True)
```

- Agora criaremos o `option_menu` para criar as p√°ginas para navega√ß√£o no topo do sidebar, com √≠cones e estilos personalizados

```python
########## MENU LATERAL #############
with st.sidebar:
    st.sidebar.image("bull-market.png", caption="Online Analytics from Fundamentus")
    #Imagem inicial
    selected = option_menu(
        menu_title=None,
        options=["Home", "Hist√≥rico", "Contato"],
        icons=["house", "bar-chart", "envelope"],
        menu_icon="cast",
        default_index=0,
        orientation="horizontal",
        styles={
            "container": {"padding": "0!important", "background-color": "#f8f9fa"},
            "nav": {"justify-content": "center"},
            "icon": {"color": "black", "font-size": "18px"},
            "nav-link": {
                "font-size": "16px",
                "text-align": "center",
                "margin": "0px",
                "--hover-color": "#eee",
                "color": "black",  # Cor do texto normal
            },
            "nav-link-selected": {
                "background-color": "#ff002f",
                "color": "black",  # Cor do texto quando selecionado
            },
        }
    )
```
- Cria√ß√£o da p√°gina "Home"
```python
if selected == "Home":
    ########## SIDEBAR ##################
    st.markdown("""
    <h1 style='text-align: center; background-color: #ff002f; color: white; padding: 10px; border-radius: 8px;'>
        Dashboard - Indicadores
    </h1>
    """, unsafe_allow_html=True)

    st.sidebar.header("Filtros")

    # Filtros primeiro!
    acoes_filtro = slidebar_filtro(dados['Papel'], "A√ß√µes")
    tipo_empresa = slidebar_filtro(dados['Tipo'], "Tipo")
    empresa = slidebar_filtro(dados['Empresa'], "Empresa")
    setor = slidebar_filtro(dados['Setor'], "Setor")
    subsetor = slidebar_filtro(dados['Subsetor'], "Subsetor")

    # Depois dos filtros, aplicamos a filtragem
    data_selection = dados.query(
        "(Papel in @acoes_filtro or @acoes_filtro == []) & "
        "(Tipo in @tipo_empresa or @tipo_empresa == []) & "
        "(Empresa in @empresa or @empresa == []) & "
        "(Setor in @setor or @setor == []) & "
        "(Subsetor in @subsetor or @subsetor == [])"
    )

    Home(data_selection)

    st.markdown("<h3 style='text-align: center;'>Rentabilidade e Retorno</h3>", unsafe_allow_html=True)
    graph(data_selection, "P/L", "LPA", "Div. Yield")

    st.markdown("<h3 style='text-align: center;'>Margens e Avalia√ß√£o</h3>", unsafe_allow_html=True)
    graph(data_selection, "Marg. EBIT", "EV / EBIT")
    graph(data_selection, "ROE", "ROIC", "EBIT / Ativo")

    st.markdown("<h3 style='text-align: center;'>Efici√™ncia Operacional</h3>", unsafe_allow_html=True)
    graph(data_selection, "Marg. Bruta", "Marg. L√≠quida", "Giro Ativos")

    st.markdown("<h3 style='text-align: center;'>Endividamento e Liquidez</h3>", unsafe_allow_html=True)
    graph(data_selection, "D√≠v. L√≠quida", "D√≠v. Bruta", "Liquidez Corr")

    st.markdown("<h3 style='text-align: center;'>Tamanho e Valor da Empresa</h3>", unsafe_allow_html=True)
    graph(data_selection, "Valor de mercado", "Valor da firma", "P/VP")

    st.markdown("<h3 style='text-align: center;'>Avalia√ß√£o por Ativos</h3>", unsafe_allow_html=True)
    graph(data_selection, "P/Ativos", "P/Cap. Giro", "P/Ativ Circ Liq")

    st.markdown("<h3 style='text-align: center;'>Desempenho Operacional</h3>", unsafe_allow_html=True)
    graph(data_selection, "Receita L√≠quida", "Lucro L√≠quido", "EBIT")

    st.markdown("<h3 style='text-align: center;'>Evolu√ß√£o dos √öltimos Anos</h3>", unsafe_allow_html=True)
    graph(data_selection, "Receita L√≠quida_1", "Lucro L√≠quido_1")
```
- Na p√°gina "Hist√≥rico", criamos os mesmos filtros da p√°gina anterior para permitir a sele√ß√£o dos dados. Em seguida, utilizamos a fun√ß√£o `melt` para criar a vari√°vel linha, que transforma as colunas de anos (como 2020, 2021, etc.) em linhas. Isso reorganiza os dados no formato ideal para construir o gr√°fico de linhas com a evolu√ß√£o das cota√ß√µes ao longo do tempo.

```python
elif selected == "Hist√≥rico":
...
    linha = pd.melt(data_selection, id_vars=['Papel'], value_vars=['2020', '2021', '2022', '2023', '2024', '2025'],
                    var_name='Ano', value_name='Valor')
    

    line_graph(linha, 'Valor')
```


üìå **Para eventuais d√∫vidas, conecte-se comigo no LinkedIn:**  
[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/samuel-alves-ribeiro-017960246/)

üìû **Ou me chame pelo whatsapp Contato** : +55 44 99174-9358
